## 任务三：注意力机制及其变体的理解与实现

### 3-1 多头注意力机制的实现

#### 原理

多头注意力机制（Multi-Head Attention, MHA）是 Transformer 模型中的核心组成部分。它通过将输入向量映射到多个不同的子空间，并在每个子空间中计算注意力，从而捕捉到更丰富的特征表示。

MHA 的主要步骤如下：

1.  **线性变换：** 将输入的 Query（Q）、Key（K）、Value（V）向量分别通过不同的线性变换矩阵（W\_q, W\_k, W\_v）映射到不同的子空间。
2.  **分割多头：** 将线性变换后的 Q、K、V 向量分割成多个头（head），每个头对应一个子空间。
3.  **计算注意力：** 在每个子空间中，计算 Q 和 K 的点积，并除以一个缩放因子（通常是 K 向量维度的平方根），然后通过 Softmax 函数得到注意力权重。
4.  **加权求和：** 使用注意力权重对 V 向量进行加权求和，得到每个子空间的输出。
5.  **合并多头：** 将所有子空间的输出拼接起来，并通过另一个线性变换矩阵（W\_o）进行映射，得到最终的输出。

#### KV Cache

KV Cache 是一种优化技术，用于加速自回归模型的推理过程。在自回归模型中，每个时间步的输出都依赖于之前所有时间步的输出。在计算注意力时，每个时间步都需要重新计算之前所有时间步的 K 和 V 向量。KV Cache 通过缓存之前时间步的 K 和 V 向量，避免了重复计算，从而提高了推理速度。

#### 代码实现 (mha.py)


#### 代码解释

*   `MultiHeadAttention` 类：
    *   `__init__` 方法：初始化线性变换层（W\_q, W\_k, W\_v, W\_o）和一些参数（d\_model, num\_heads, d\_k）。
    *   `split_heads` 方法：将输入的向量分割成多个头。
    *   `forward` 方法：执行多头注意力计算。包括线性变换、分割多头、计算注意力分数、加权求和、合并多头、最后的线性变换等步骤。处理了 KV Cache 的逻辑。
*   `test_attention` 函数：
    *   创建了一个 `MultiHeadAttention` 实例。
    *   生成了一个随机矩阵 `x` 作为输入。
    *   测试了普通前向传播和 KV Cache 的使用。

### 3-2 注意力机制变体的实现

#### GQA (Grouped-Query Attention)

GQA 将查询头（query heads）分组，每组共享相同的键头（key head）和值头（value head）。这样可以减少 KV Cache 的大小，同时保持接近 MHA 的性能。

#### MQA (Multi-Query Attention)

MQA 是 GQA 的一个极端情况，所有查询头共享同一个键头和值头。MQA 进一步减少了 KV Cache 的大小，但可能会牺牲一些性能。

#### 代码实现 (gqa_and_mqa.py)