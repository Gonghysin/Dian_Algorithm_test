# 数据爬取
- 题目给我爬虫脚本文件只能爬取在排行榜中前10番剧的评论，这样得到的评论一般都是给分比较高的。高分评论过多，这样就会在数据层面产生偏见，模型对于低分评价的泛化能力就会降低。
- 所以我大量修改了爬虫脚本，使其可以分别在高、中、低三个分段中，随机选择番剧，然后再爬取评论。这样可以使得评分尽可能的分散在整个区间中。
- 三个分段的评论、评分，各自生成一个数据集（high/medium/low），然后合并(combined)，并绘制频率分布图。并与原本爬虫脚本生成的数据集，生成的频率分布图进行比较。可以看到，新的数据集在3-7分的分段多了很多样本，有助于模型学习中低分段的评论，增强泛化能力。

原本的爬虫文件爬取数据集的评分频率分布
![原本的爬虫文件爬取数据集的评分频率分布](https://hysinoss-1334037784.cos.ap-shanghai.myqcloud.com/test/single_rating_distribution.png)

修改后爬虫脚本获取的数据集评分频率分布
![修改后爬虫脚本获取的数据集评分频率分布](https://hysinoss-1334037784.cos.ap-shanghai.myqcloud.com/test/rating_distribution.png)
# 数据清洗
#### **清洗评论文本**
- **问题**：评论可能包含无关字符、重复内容或格式问题。
- **去除特殊字符和多余空格**：例如标点符号、换行符、表情符号（视情况保留有意义的表情）。
#### **处理不平衡数据（可选）**
- **问题**：评分分布可能不均匀（例如大部分是8-10分，少量低分）。
- **操作**：考虑下采样高频评分或上采样低频评分（例如用SMOTE）。
- **我打算分别尝试处理、与不处理，这两者训练出来的模型哪一个效果更好一些**
#### **数据去重**
- **问题**：重复的评论和评分对可能影响模型泛化能力。
- **操作**：删除完全相同的评论-评分对。
#### 使用LLM批处理进行数据清洗（可选）
- **我将分别尝试使用传统方法，或者使用LLM进行数据清洗，并对比训练结果**

清洗后的数据集名称：cleaned_comments_and_ratings.csv

# 数据预处理
####  **文本分词与Tokenization**
- **工具**：使用`transformers`库中与`bert-base-chinese`配套的`BertTokenizer`。
- **注意**：
    - 中文BERT使用字级别分词（而不是词级别），无需额外分词工具（如jieba）。
    - 输入长度有限（最大512个token），长评论需要截断。
#### **标签处理**
- 将评分（1-10）归一化到[0, 1]，便于模型输出。
#### 划分数据集
- 按8:1:1划分训练集、验证集和测试集
- 确保评分分布在各子集间尽量一致（分层抽样）。

训练集、验证集名称：train_set.csv、val_set.csv

# 数据爬取
// ... existing code ...

# 数据清洗
// ... existing code ...

# 数据预处理
// ... existing code ...

# 模型微调
对预训练的中文BERT模型（bert-base-chinese）进行微调训练，使用PyTorch实现了一个简易但高效的微调程序。

### 模型架构设计
- **基础模型**：使用预训练的`bert-base-chinese`作为特征提取器
- **任务头设计**：
  - 在BERT输出（`[CLS]` token的表示）后添加多层回归器
  - 回归器结构：Dropout(0.3) → Linear(768→128) → LayerNorm → GELU → Dropout(0.3) → Linear(128→1) → Sigmoid
  - 使用Sigmoid激活函数确保输出在[0,1]范围内，对应归一化后的评分

### 损失函数设计
- **加权MSE损失**：专门设计了`WeightedMSELoss`类，对低评分样本（≤0.3，对应原始评分1-3分）给予更高权重（默认2.0倍）
- 这种设计有效解决了数据集中低评分样本较少导致的"偏科"问题

### 超参数设计
- **学习率**：使用`2e-5`，适合BERT微调的回归任务
- **Batch size**：32，根据GPU显存优化
- **训练轮次**：最大15轮，配合早停机制
- **权重衰减**：0.01，提供L2正则化以防止过拟合
- **Dropout率**：0.3，比标准的0.1更高，增强正则化效果

### 训练优化策略
- **早停机制**：
  - 当验证损失连续3轮未改善时停止训练
  - 保存验证损失最低的模型权重
  - 训练结束后恢复最佳模型状态
- **学习率调度**：
  - 使用`ReduceLROnPlateau`调度器
  - 当验证损失连续2轮未改善时，将学习率减半
  - 有效避免训练后期在局部最小值震荡

### 评估指标
- **主要指标**：
  - MSE（均方误差）：整体预测准确度
  - RMSE（均方根误差）：与原始评分尺度相同的误差
  - MAE（平均绝对误差）：预测偏差的平均大小
  - Spearman相关系数：评估预测值与真实值的排序一致性
- **特殊关注**：
  - 低评分样本MSE：单独监控评分≤0.3样本的MSE
  - 通过可视化散点图中将低评分样本标红，直观评估模型对低分样本的预测效果

### 可视化与监控
- **训练过程可视化**：
  - 损失曲线：训练损失和验证损失
  - 评估指标曲线：MSE、RMSE、MAE、Spearman相关系数
  - 低评分样本MSE曲线：监控模型对低分样本的预测能力
  - 预测vs真实值散点图：直观展示预测质量，低评分样本用红色标注
  - 评分分布对比：比较预测评分与真实评分的分布差异
- **最佳模型标记**：在所有图表中用垂直虚线标记最佳模型所在轮次

### 灵活性设计
- **数据采样选项**：支持使用全部数据或仅5%数据进行快速测试
- **模型保存**：自动保存最佳模型权重到`task2/models/simple_model.pth`
- **训练历史记录**：将训练过程的可视化结果保存为图表

### 模型测试
- 训练完成后，使用三个典型评论测试模型：
  - 积极评论："这部动漫太棒了，情节紧凑，人物刻画深入，强烈推荐！"
  - 中性评论："剧情一般，画风还可以，打发时间可以看看。"
  - 负面评论："太难看了，浪费时间，剧情混乱，角色塑造差。"
- 测试结果显示模型能够准确区分不同情感倾向的评论，并给出合理的评分预测

